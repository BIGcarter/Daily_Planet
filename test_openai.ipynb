{
 "cells": [
  {
   "cell_type": "code",
   "execution_count": 101,
   "id": "b996143f",
   "metadata": {},
   "outputs": [],
   "source": [
    "import os\n",
    "import re\n",
    "import textwrap\n",
    "import datetime as dt\n",
    "from pathlib import Path\n",
    "from typing import List\n",
    "\n",
    "import requests\n",
    "from bs4 import BeautifulSoup  # type: ignore\n",
    "import arxiv  # arxiv API wrapper\n",
    "import openai\n",
    "import time\n",
    "\n",
    "from __future__ import annotations\n",
    "\n",
    "client = OpenAI(api_key=\"sk-psdwxdblqsanpckjwnkoebqnvpgszjdahyvqpgbexltvrkpx\", \n",
    "                base_url=\"https://api.siliconflow.cn/v1\",\n",
    "                )\n",
    "\n",
    "MODEL = \"Qwen/Qwen2.5-32B-Instruct\"  \n",
    "\n",
    "\n",
    "ARXIV_NEW_URL = \"https://arxiv.org/list/astro-ph/new\"\n",
    "BATCH_SIZE_API = 20      # arXiv API batch size\n",
    "BATCH_SIZE_SUM = 10      # #abstracts per summarisation call\n",
    "MAX_RETRIES = 3          # API retries\n",
    "RETRY_WAIT = 4           # seconds between retries\n",
    "MAX_PAPERS = 20\n",
    "\n",
    "# TOPIC = \"Star formation / Planet formation: including star formation in the Milky Way, and the formation of protostellar/protoplanetary disks, planetary systems around stars. Both theoretical and observational papers.\"\n",
    "\n",
    "TOPIC = \"binary star\"\n",
    "\n",
    "OUTPUT_DIR = './'"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "56464573",
   "metadata": {},
   "outputs": [],
   "source": [
    "classify_sysprompt = (\n",
    "    \"You are an expert astrophysics librarian.Your job is to determine whether a \"\n",
    "    \"scientific paper is related to a given research topic based solely on its title.\\n\\n\"\n",
    "    \"Instructions:\\n\"\n",
    "    \"- Use your domain knowledge in astrophysics to make a judgment.\\n\"\n",
    "    \"- Only respond with \\\"Yes\\\" or \\\"No\\\".\\n\"\n",
    "    \"- Be conservative: if the relation is unclear or indirect, respond \\\"No\\\".\\n\"\n",
    "    \"- Do not explain or elaborate.\"\n",
    ")\n",
    "\n",
    "classify_userprompt_tpl = (\n",
    "    \"Topic: {TOPIC}\\nTitle: {title}\\nDoes the title belong to the topic above?\"\n",
    ")\n",
    "\n",
    "summarize_sysprompt = (\n",
    "    \"You are an expert assistant for academic summarization and translation, \"\n",
    "    \"specializing in astrophysics papers. \\n\"\n",
    "    \"Your task is to read English abstracts and generate accurate, concise, and \"\n",
    "    \"objective Chinese summaries.\"\n",
    ")\n",
    "\n",
    "summarize_userprompt_header = (\n",
    "    \"Summarize the following astrophysics paper abstracts in Chinese. For each \"\n",
    "    \"abstract, produce a 4‑6 sentence summary, and extract 3-4 keywords, following these rules:\\n\"\n",
    "    \"1. The summary must remain strictly objective. Do not include any subjective \"\n",
    "    \"opinions or speculative phrases such as “对……研究具有重要意义” or “为……提供了重要参考”.\\n\"\n",
    "    \"2. Avoid generic or formulaic language; use concise and precise academic Chinese.\\n\"\n",
    "    \"3. Ensure key methods, results, and research subjects are not omitted.\\n\\n\"\n",
    "    \"Return your answer in the format:\\n\"\n",
    "    \"### <arXiv ID>\\n<summary>\\n\"\n",
    ")\n",
    "# ───────────────────────────────────────────────\n",
    "\n",
    "# ╭───────────────────────── helper functions ──────────────────────────╮\n",
    "\n",
    "def fetch_new_submission_entries(url: str = ARXIV_NEW_URL) -> List[Dict[str, str]]:\n",
    "    \"\"\"Return list of dicts with 'id' and 'title' from *New submissions* section.\"\"\"\n",
    "    resp = requests.get(url, timeout=30)\n",
    "    resp.raise_for_status()\n",
    "    soup = BeautifulSoup(resp.text, \"html.parser\")\n",
    "\n",
    "    header = soup.find(\"h3\", string=lambda s: s and \"new submissions\" in s.lower())\n",
    "    if not header:\n",
    "        raise RuntimeError(\"Cannot locate 'New submissions' header – page structure changed.\")\n",
    "\n",
    "    dl = header.find_parent(\"dl\") or header.find_next(\"dl\")\n",
    "    if not dl:\n",
    "        raise RuntimeError(\"Cannot find <dl> container for new submissions.\")\n",
    "\n",
    "    id_regex = re.compile(r\"\\d{4}\\.\\d{4,5}\")\n",
    "    entries: List[Dict[str, str]] = []\n",
    "\n",
    "    dt_tags = dl.find_all(\"dt\", recursive=False)\n",
    "    dd_tags = dl.find_all(\"dd\", recursive=False)\n",
    "    for dt_tag, dd_tag in zip(dt_tags, dd_tags):\n",
    "        link = dt_tag.find(\"a\", href=re.compile(r\"/abs/\"))\n",
    "        if not (link and link[\"href\"]):\n",
    "            continue\n",
    "        m = id_regex.search(link[\"href\"])\n",
    "        if not m:\n",
    "            continue\n",
    "        arxiv_id = m.group(0)\n",
    "\n",
    "        title_div = dd_tag.find(\"div\", class_=re.compile(r\"list-title\"))\n",
    "        if not title_div:\n",
    "            continue\n",
    "        title_text = title_div.get_text(separator=\" \", strip=True)\n",
    "        title_text = title_text.replace(\"Title:\", \"\").strip()\n",
    "\n",
    "        entries.append({\"id\": arxiv_id, \"title\": title_text})\n",
    "        if len(entries) >= MAX_PAPERS:\n",
    "            break\n",
    "    return entries\n",
    "\n",
    "\n",
    "def classify_title(title: str) -> bool:\n",
    "    \"\"\"LLM Yes/No using conservative astrophysics librarian prompt.\"\"\"\n",
    "    user_prompt = classify_userprompt_tpl.format(TOPIC=TOPIC, title=title)\n",
    "    resp = client.chat.completions.create(\n",
    "        model=MODEL,\n",
    "        messages=[\n",
    "            {\"role\": \"system\", \"content\": classify_sysprompt},\n",
    "            {\"role\": \"user\", \"content\": user_prompt},\n",
    "        ],\n",
    "        temperature=0,\n",
    "    )\n",
    "    return resp.choices[0].message.content.strip().lower().startswith(\"y\")\n",
    "\n",
    "\n",
    "def classify_titles(entries: List[Dict[str, str]]) -> List[str]:\n",
    "    \"\"\"Loop over titles; return list of IDs whose titles get 'Yes'.\"\"\"\n",
    "    matched_ids: List[str] = []\n",
    "    for entry in entries:\n",
    "        if classify_title(entry[\"title\"]):\n",
    "            matched_ids.append(entry[\"id\"])\n",
    "    return matched_ids\n",
    "\n",
    "\n",
    "def get_metadata_for_ids(ids: List[str]) -> List[arxiv.Result]:\n",
    "    \"\"\"Fetch metadata via arXiv API in batches with retry/back‑off.\"\"\"\n",
    "    if not ids:\n",
    "        return []\n",
    "\n",
    "    client = arxiv.Client()\n",
    "    results: List[arxiv.Result] = []\n",
    "    for start in range(0, len(ids), BATCH_SIZE_API):\n",
    "        batch = ids[start : start + BATCH_SIZE_API]\n",
    "        retries = MAX_RETRIES\n",
    "        while retries:\n",
    "            try:\n",
    "                search = arxiv.Search(id_list=batch, max_results=len(batch))\n",
    "                results.extend(list(client.results(search)))\n",
    "                break\n",
    "            except Exception as e:\n",
    "                retries -= 1\n",
    "                if retries == 0:\n",
    "                    print(f\"[WARN] Failed batch {batch}: {e}\")\n",
    "                else:\n",
    "                    wait = RETRY_WAIT * (MAX_RETRIES - retries)\n",
    "                    print(f\"[INFO] Retry in {wait}s … ({retries} left)\")\n",
    "                    time.sleep(wait)\n",
    "    return results\n",
    "\n",
    "\n",
    "def batch_summarise(papers: List[arxiv.Result]) -> List[str]:\n",
    "    \"\"\"Summarise abstracts in batches; return list of summaries aligned to papers.\"\"\"\n",
    "    summaries: List[str] = []\n",
    "    for start in range(0, len(papers), BATCH_SIZE_SUM):\n",
    "        batch = papers[start : start + BATCH_SIZE_SUM]\n",
    "        # Build user prompt with multiple abstracts\n",
    "        parts = []\n",
    "        for p in batch:\n",
    "            parts.append(f\"### {p.get_short_id()}\\n{p.summary.strip()}\")\n",
    "        user_prompt = summarize_userprompt_header + \"\\n\\n\" + \"\\n\\n\".join(parts)\n",
    "\n",
    "        resp = client.chat.completions.create(\n",
    "            model=MODEL,\n",
    "            messages=[\n",
    "                {\"role\": \"system\", \"content\": summarize_sysprompt},\n",
    "                {\"role\": \"user\", \"content\": user_prompt},\n",
    "            ],\n",
    "            temperature=0.3,\n",
    "        )\n",
    "        # Parse response: expect blocks \"### ID\\nsummary\".\n",
    "        text = resp.choices[0].message.content.strip()\n",
    "        # Simple split: assume summaries appear in same order\n",
    "        blocks = [b.strip() for b in text.split(\"###\") if b.strip()]\n",
    "        for block in blocks:\n",
    "            summaries.append(block.split(\"\\n\", 1)[1].strip() if \"\\n\" in block else block)\n",
    "    return summaries[: len(papers)]\n",
    "\n",
    "\n",
    "def build_markdown(papers: List[arxiv.Result], summaries: List[str], date: dt.date) -> str:\n",
    "    lines = [f\"# Daily Planet 行星日报 \\n{date}\\n\"]\n",
    "    if not papers:\n",
    "        lines.append(\"*(No matching papers today.)*\\n\")\n",
    "    for paper, summ in zip(papers, summaries):\n",
    "        authors = \", \".join(a.name for a in paper.authors)\n",
    "\n",
    "        # 提取摘要和关键词\n",
    "        keywords = \"\"\n",
    "        if \"关键词：\" in summ:\n",
    "            parts = summ.rsplit(\"关键词：\", 1)\n",
    "            summary_body = parts[0].strip()\n",
    "            keywords = parts[1].strip()\n",
    "        else:\n",
    "            summary_body = summ.strip()\n",
    "\n",
    "        lines.append(\n",
    "            f\"### [{paper.title}]({paper.entry_id})\\n\"\n",
    "            f\"**Authors**: {authors}\\n\\n\"\n",
    "            f\"**摘要**:\\n{summary_body}\\n\"\n",
    "        )\n",
    "        if keywords:\n",
    "            lines.append(f\"**关键词**: {keywords}\\n\")\n",
    "    return \"\\n\".join(lines)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 102,
   "id": "0741e1ed",
   "metadata": {},
   "outputs": [],
   "source": [
    "entries = fetch_new_submission_entries()\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 103,
   "id": "38919f88",
   "metadata": {},
   "outputs": [],
   "source": [
    "matched_ids = classify_titles(entries)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 104,
   "id": "3dc7de7a",
   "metadata": {},
   "outputs": [],
   "source": [
    "papers = get_metadata_for_ids(matched_ids)\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 116,
   "id": "ca8e7a97",
   "metadata": {},
   "outputs": [],
   "source": [
    "summaries = batch_summarise(papers)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 129,
   "id": "ae133e40",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[INFO] Wrote astro_ph_daily_picks/astro_ph_2025-05-27.md with 3 summaries → astro_ph_daily_picks/astro_ph_2025-05-27.md\n"
     ]
    }
   ],
   "source": [
    "target_date = datetime.now().date()\n",
    "OUTPUT_DIR = './astro_ph_daily_picks'\n",
    "md = build_markdown(papers, summaries, target_date)\n",
    "\n",
    "out_dir = Path(OUTPUT_DIR); out_dir.mkdir(parents=True, exist_ok=True)\n",
    "outfile = out_dir / f\"astro_ph_{target_date}.md\"\n",
    "outfile.write_text(md, encoding=\"utf-8\")\n",
    "print(f\"[INFO] Wrote {outfile} with {len(summaries)} summaries → {outfile}\")"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "4a25020c",
   "metadata": {},
   "outputs": [],
   "source": []
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "paperagent",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.10.17"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
